{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_y527G0bTsM"
      },
      "source": [
        "\n",
        "\n",
        " # **Imports and Setup**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PcrJQvZbAex",
        "outputId": "40f48d26-3577-465c-e6b3-68bf072527df"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, auc)\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Environment setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN-cf6GQbdsR"
      },
      "source": [
        "# **Load MIMII Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqygBwf8bqt3",
        "outputId": "161f438c-d3f5-4fd9-e1fd-d604014b0811"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Base directory for all pump data\n",
        "# BASE_PATH = \"/content/drive/My Drive/TractorCare/data/pump/\"\n",
        "BASE_PATH = \"../data/pump/\"\n",
        "\n",
        "# Machine IDs\n",
        "machine_ids = [\"id_00\", \"id_02\", \"id_04\", \"id_06\"]\n",
        "\n",
        "normal_files = []\n",
        "abnormal_files = []\n",
        "\n",
        "# Loop through each machine ID and collect .wav files\n",
        "for mid in machine_ids:\n",
        "    normal_dir = os.path.join(BASE_PATH, mid, \"normal\")\n",
        "    abnormal_dir = os.path.join(BASE_PATH, mid, \"abnormal\")\n",
        "\n",
        "    # Check existence before listing\n",
        "    if os.path.exists(normal_dir):\n",
        "        normal_files.extend(\n",
        "            [os.path.join(normal_dir, f) for f in os.listdir(normal_dir) if f.endswith('.wav')]\n",
        "        )\n",
        "    if os.path.exists(abnormal_dir):\n",
        "        abnormal_files.extend(\n",
        "            [os.path.join(abnormal_dir, f) for f in os.listdir(abnormal_dir) if f.endswith('.wav')]\n",
        "        )\n",
        "\n",
        "print(f\"Normal samples found: {len(normal_files)}\")\n",
        "print(f\"Abnormal samples found: {len(abnormal_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dbb4e26"
      },
      "source": [
        "## **Handling Multiple Data Sources & Future Data**\n",
        "\n",
        "While this notebook currently utilizes the MIMII Dataset for pump sounds, the approach is designed to be adaptable for integrating data from multiple sources, such as the upcoming tractor sound dataset.\n",
        "\n",
        "When incorporating new audio data sources or additional data over time, the following strategy will be employed:\n",
        "\n",
        "1.  **Consistent Feature Extraction:** The `extract_mfcc_features` function, or a similar standardized feature extraction pipeline, will be applied uniformly to all audio files from every source. This ensures that the extracted features have consistent dimensions and meaning across the entire dataset.\n",
        "2.  **Data Merging/Concatenation:** The extracted feature arrays from different sources will be combined into a single, unified dataset. This typically involves concatenating the feature arrays (e.g., using `numpy.vstack` or similar) and creating corresponding labels that identify the source and condition (normal/abnormal).\n",
        "3.  **Handling Data Duplication:** Before merging, steps will be taken to identify and handle potential data duplication if the new dataset overlaps with existing data. This might involve checking file metadata, timestamps, or even comparing feature vectors for near-duplicates.\n",
        "4.  **Re-balancing (if necessary):** After merging, the class distribution (normal vs. abnormal) will be re-evaluated. If the combined dataset is imbalanced, resampling techniques (like the undersampling used previously, or oversampling) will be applied again to ensure a balanced dataset for model training.\n",
        "5.  **Source as a Feature (Optional):** In some cases, the data source itself could be treated as an additional feature or used in a multi-task learning setup if there are distinct characteristics between sources.\n",
        "\n",
        "This modular approach ensures that the machine learning models can be trained on a comprehensive dataset, regardless of the number or origin of the audio sources, while maintaining data integrity and handling potential issues like imbalance and duplication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fGJui1Vbz9f"
      },
      "source": [
        "# **Data Visualization - Waveforms**\n",
        "Show how loud the sound is changing over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "QTApJlTyb39t",
        "outputId": "401b83a5-bef3-4eb1-9ef9-356911774806"
      },
      "outputs": [],
      "source": [
        "# Visualize sample waveforms (use first 2 from each class)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
        "\n",
        "# Normal samples\n",
        "for i in range(2):\n",
        "    audio, sr = librosa.load(normal_files[i], sr=16000, duration=3)\n",
        "    axes[0, i].plot(audio)\n",
        "    axes[0, i].set_title(f'Normal Pump Sample {i+1}')\n",
        "    axes[0, i].set_xlabel('Time (samples)')\n",
        "    axes[0, i].set_ylabel('Amplitude')\n",
        "\n",
        "# Abnormal samples\n",
        "for i in range(2):\n",
        "    audio, sr = librosa.load(abnormal_files[i], sr=16000, duration=3)\n",
        "    axes[1, i].plot(audio)\n",
        "    axes[1, i].set_title(f'Abnormal Pump Sample {i+1}')\n",
        "    axes[1, i].set_xlabel('Time (samples)')\n",
        "    axes[1, i].set_ylabel('Amplitude')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('waveforms_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzz9_bSrb9Ts"
      },
      "source": [
        "# **Spectrograms**\n",
        "Show which frequencies are present in the sound and how their loudness changes over time. Useful for seeing patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "Ig7modkyb_6j",
        "outputId": "a5cd2dab-c4c1-4c6d-c452-9c6733e5b5e1"
      },
      "outputs": [],
      "source": [
        "# Visualize spectrograms\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
        "\n",
        "# Normal\n",
        "for i in range(2):\n",
        "    audio, sr = librosa.load(normal_files[i], sr=16000, duration=3)\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[0, i])\n",
        "    axes[0, i].set_title(f'Normal Spectrogram {i+1}')\n",
        "\n",
        "# Abnormal\n",
        "for i in range(2):\n",
        "    audio, sr = librosa.load(abnormal_files[i], sr=16000, duration=3)\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
        "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[1, i])\n",
        "    axes[1, i].set_title(f'Abnormal Spectrogram {i+1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('spectrograms_comparison.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10b8117e"
      },
      "source": [
        "# **Noise Reduction Considerations**\n",
        "\n",
        "Noise in real-world audio can hinder anomaly detection by masking true anomalies or creating false ones. Applying noise reduction is crucial, especially with new, potentially noisy data. Techniques include:\n",
        "\n",
        "1.  **Spectral Gating:** Suppressing frequencies below a certain loudness threshold.\n",
        "2.  **Noise Profiling:** Subtracting a recorded noise sample from the audio.\n",
        "3.  **Deep Learning-based Denoising:** Using neural networks trained to remove noise.\n",
        "\n",
        "Incorporating filtering (like high-pass or low-pass) or other techniques can be beneficial for noisy data like tractor sounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zSTJL2pcO1f"
      },
      "source": [
        "# **Feature Extraction explanation**\n",
        "\n",
        "Mel-Frequency Cepstral Coefficients (MFCCs) are widely used features in audio processing, particularly for tasks like speech recognition and music genre classification. They effectively capture the spectral envelope of a sound, which is crucial for distinguishing different audio events or sources. This makes them suitable for identifying distinct patterns between 'normal' and 'abnormal' pump sounds.\n",
        "\n",
        "The `extract_mfcc_features` function includes a `max_len` parameter. This parameter is used to ensure that all extracted MFCC feature arrays have a fixed size. This process provides a uniform input size for the models, allowing them to process the data effectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKZ6o5wcNBn",
        "outputId": "a37923f1-100f-4711-8c4b-064da220a0d7"
      },
      "outputs": [],
      "source": [
        "def extract_mfcc_features(file_path, n_mfcc=40, max_len=100):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from audio file with an optional high-pass filter.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: Path to the audio file.\n",
        "    - n_mfcc: Number of MFCC coefficients to extract.\n",
        "    - max_len: Maximum length of the MFCC sequence.\n",
        "\n",
        "    Returns:\n",
        "    - mfcc: numpy array of shape (n_mfcc, max_len) or None if processing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio\n",
        "        audio, sr = librosa.load(file_path, sr=16000, duration=10)\n",
        "\n",
        "        # Apply a simple high-pass filter (e.g., cutoff at 100 Hz)\n",
        "        # This is a basic implementation, more sophisticated filters might be needed.\n",
        "        from scipy.signal import butter, filtfilt\n",
        "        nyquist = 0.5 * sr\n",
        "        cutoff = 100  # Hz\n",
        "        normal_cutoff = cutoff / nyquist\n",
        "        # Get the filter coefficients\n",
        "        b, a = butter(5, normal_cutoff, btype='high', analog=False)\n",
        "        # Apply the filter\n",
        "        audio = filtfilt(b, a, audio)\n",
        "\n",
        "\n",
        "        # Extract MFCCs\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "        # Pad or truncate to fixed length\n",
        "        if mfcc.shape[1] < max_len:\n",
        "            pad_width = max_len - mfcc.shape[1]\n",
        "            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mfcc = mfcc[:, :max_len]\n",
        "\n",
        "        return mfcc\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract features for visualization samples only\n",
        "print(\"Extracting MFCC features for visualization...\")\n",
        "X_normal_sample = []\n",
        "X_abnormal_sample = []\n",
        "\n",
        "for file in normal_files[:5]:  # Just for visualization\n",
        "    mfcc = extract_mfcc_features(file)\n",
        "    if mfcc is not None:\n",
        "        X_normal_sample.append(mfcc)\n",
        "\n",
        "for file in abnormal_files[:5]:\n",
        "    mfcc = extract_mfcc_features(file)\n",
        "    if mfcc is not None:\n",
        "        X_abnormal_sample.append(mfcc)\n",
        "\n",
        "print(f\" Extracted {len(X_normal_sample)} normal and {len(X_abnormal_sample)} abnormal samples for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGP3CvGOcj8e"
      },
      "source": [
        "# **Visualize MFCC Features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "FXCiZ-Skcn_F",
        "outputId": "75482c17-3c0f-4f91-93aa-df17315a009f"
      },
      "outputs": [],
      "source": [
        "# Show sample MFCC\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
        "\n",
        "librosa.display.specshow(X_normal_sample[0], x_axis='time', ax=axes[0], cmap='viridis')\n",
        "axes[0].set_title('Normal Pump MFCC Features', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('MFCC Coefficients')\n",
        "axes[0].set_xlabel('Time Frames')\n",
        "\n",
        "librosa.display.specshow(X_abnormal_sample[0], x_axis='time', ax=axes[1], cmap='viridis')\n",
        "axes[1].set_title('Abnormal Pump MFCC Features', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('MFCC Coefficients')\n",
        "axes[1].set_xlabel('Time Frames')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('mfcc_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYSkXQQcs3T"
      },
      "source": [
        "# **Handle Class Imbalance + Extract Features from ALL files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324,
          "referenced_widgets": [
            "21d99b4b435a4e0ebb593c1d692905be",
            "299f309f8598437e8647daa884b67b22",
            "2d61e365ee75476eb7c2227a7afb1cae",
            "316eafc24ff84e539a8c2b55a73ec2a3",
            "98c31ab3d5a64fda9c44d9d48c63e467",
            "5f8f3c5f74be4d8bb951313168ea968f",
            "18a738c8af9a4361bf333e024ff490b3",
            "cc068a5f5fad4b5aa62c2ae80ac3854e",
            "0d11eb90574e4b9a937946f283a8d6f7",
            "13ce43d1e6c34a6daeea0100557c3cb7",
            "313790d1ee6648b0b70611818fa283a9",
            "71d6357a0b7b4cd9b414f1b670890224",
            "630a1c02e002442298a19e5563b362df",
            "a41cb4f10153443a82c543c4ac414347",
            "a55f2fac53424d0397f6191d79c45dbc",
            "14d7b6d6a95f48a8a9d6571d3825e219",
            "421c068782d94f85ab2572deade4d27e",
            "79157940f5bd4123b3ad4ff3db5b85cc",
            "0410b1dfeb1d42b9948a3ff180fd130a",
            "4448aaf7b084477f8df736e3cee545c6",
            "7a79f69f3e914a0ea0a7f8f59ed91224",
            "2796fa132f5747c09bcd3b57b9d9037c"
          ]
        },
        "id": "2tVzGy4rc6mR",
        "outputId": "16d8e79d-e823-405d-d630-13e9e42f0509"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Balance the dataset\n",
        "# Option 1: Undersample normal to match abnormal\n",
        "normal_sample_size = len(abnormal_files)  # 456\n",
        "normal_files_balanced = resample(normal_files,\n",
        "                                 n_samples=normal_sample_size,\n",
        "                                 random_state=42)\n",
        "\n",
        "print(f\"Balanced dataset:\")\n",
        "print(f\"  Normal: {len(normal_files_balanced)}\")\n",
        "print(f\"  Abnormal: {len(abnormal_files)}\")\n",
        "\n",
        "def extract_mfcc_features(file_path, n_mfcc=40, max_len=100):\n",
        "    \"\"\"Extract MFCC features from audio file\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(file_path, sr=16000, duration=10)\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "        if mfcc.shape[1] < max_len:\n",
        "            pad_width = max_len - mfcc.shape[1]\n",
        "            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mfcc = mfcc[:, :max_len]\n",
        "\n",
        "        return mfcc\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Extract features from BALANCED dataset\n",
        "print(\"\\nExtracting MFCC features from all balanced samples...\")\n",
        "X_normal = []\n",
        "X_abnormal = []\n",
        "\n",
        "# Use all abnormal (456) and balanced normal (456)\n",
        "print(\"\\nProcessing normal samples...\")\n",
        "for file in tqdm(normal_files_balanced, desc=\"Normal\"):\n",
        "    mfcc = extract_mfcc_features(file)\n",
        "    if mfcc is not None:\n",
        "        X_normal.append(mfcc)\n",
        "\n",
        "print(\"\\nProcessing abnormal samples...\")\n",
        "for file in tqdm(abnormal_files, desc=\"Abnormal\"):\n",
        "    mfcc = extract_mfcc_features(file)\n",
        "    if mfcc is not None:\n",
        "        X_abnormal.append(mfcc)\n",
        "\n",
        "print(f\"\\n Feature extraction complete!\")\n",
        "print(f\"  Normal samples: {len(X_normal)}\")\n",
        "print(f\"  Abnormal samples: {len(X_abnormal)}\")\n",
        "print(f\"  Total dataset size: {len(X_normal) + len(X_abnormal)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY5vN-Utd-jd"
      },
      "source": [
        "# **Noise Reduction Considerations**\n",
        "\n",
        "Real-world audio data is often contaminated with various types of noise, such as background machinery sounds, environmental noise, or electrical interference. For anomaly detection tasks like identifying abnormal pump sounds, the presence of significant noise can make it challenging for models to distinguish between genuine anomalies and noisy normal samples. Noise can mask subtle acoustic cues that characterize an anomaly or introduce spurious patterns that the model might incorrectly interpret as abnormal. Therefore, applying noise reduction techniques can be a crucial preprocessing step, especially when dealing with a new dataset where the noise characteristics might differ from the training data.\n",
        "\n",
        "Several techniques can be employed for noise reduction in audio:\n",
        "\n",
        "1.  **Spectral Gating:** This method works by identifying frequency components that are significantly louder than the surrounding noise in the frequency domain. It then \"gates\" or reduces the amplitude of frequencies that are below a certain threshold, effectively suppressing background noise while preserving the dominant signal.\n",
        "2.  **Noise Profiling:** This involves capturing a sample of the background noise when the target sound (e.g., the pump) is not present. This noise profile is then subtracted from the entire audio signal to remove the estimated noise components. This is effective when the noise is relatively stationary.\n",
        "3.  **Deep Learning-based Denoising:** More advanced techniques utilize neural networks trained on large datasets of noisy and clean audio to learn how to remove noise. Autoencoders or U-Net architectures are commonly used for this purpose. These methods can be powerful for complex and non-stationary noise but require significant data and computational resources for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKCNaqGyeHon"
      },
      "outputs": [],
      "source": [
        "# Placeholder for Noise Reduction Implementation\n",
        "print(\"Placeholder for noise reduction implementation added.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj4t7RLaeXWn"
      },
      "source": [
        "# **Feature aggregation for traditional models**\n",
        "##**Prepare Data for Traditional ML**\n",
        "\n",
        "For traditional machine learning algorithms like K-Nearest Neighbors, and Support Vector Machines, the input data typically needs to be in a fixed-size vector format. Unlike some deep learning architectures that can directly process sequential or multi-dimensional data (like the CNN using the 2D MFCC arrays), these models require a consistent number of features for each sample.\n",
        "\n",
        "The extracted MFCC features for each audio file have a shape of `(n_mfcc, max_len)`, which is a 2D array. Although we padded or truncated to a `max_len`, the time dimension (the second dimension) still varies slightly depending on the exact length of the audio after loading, or more importantly, traditional models cannot directly handle this 2D grid structure as input.\n",
        "\n",
        "To convert these 2D MFCC arrays into a fixed-size 1D vector suitable for traditional ML, we compute statistics across the time dimension (`axis=1`). The mean and standard deviation are commonly used statistical measures for this purpose.\n",
        "\n",
        "-   The **mean** of each MFCC coefficient across all time frames captures the average spectral characteristic over the duration of the audio segment.\n",
        "-   The **standard deviation** of each MFCC coefficient across all time frames captures the variability or temporal dynamics of that spectral characteristic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJvn9UVce_M2",
        "outputId": "4f0d3b29-5677-426f-92fa-3fa6da7b4be8"
      },
      "outputs": [],
      "source": [
        "# For traditional ML (not deep learning), compute statistics\n",
        "def compute_statistics(mfcc):\n",
        "    \"\"\"Compute mean and std of MFCC coefficients\"\"\"\n",
        "    return np.concatenate([np.mean(mfcc, axis=1), np.std(mfcc, axis=1)])\n",
        "\n",
        "X_normal_stats = np.array([compute_statistics(x) for x in X_normal])\n",
        "X_abnormal_stats = np.array([compute_statistics(x) for x in X_abnormal])\n",
        "\n",
        "# Combine and create labels\n",
        "X_traditional = np.vstack([X_normal_stats, X_abnormal_stats])\n",
        "y_traditional = np.concatenate([np.zeros(len(X_normal_stats)),\n",
        "                                np.ones(len(X_abnormal_stats))])\n",
        "\n",
        "# Split data\n",
        "X_train_trad, X_test_trad, y_train_trad, y_test_trad = train_test_split(\n",
        "    X_traditional, y_traditional, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_trad = scaler.fit_transform(X_train_trad)\n",
        "X_test_trad = scaler.transform(X_test_trad)\n",
        "\n",
        "print(f\"Training set (Traditional ML): {X_train_trad.shape}\")\n",
        "print(f\"Test set (Traditional ML): {X_test_trad.shape}\")\n",
        "\n",
        "\n",
        "# Algorithm 1: K-Nearest Neighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "print(\"\\nTraining KNN...\")\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_trad, y_train_trad)\n",
        "knn_pred = knn_model.predict(X_test_trad)\n",
        "\n",
        "knn_results = {\n",
        "    'accuracy': accuracy_score(y_test_trad, knn_pred),\n",
        "    'precision': precision_score(y_test_trad, knn_pred),\n",
        "    'recall': recall_score(y_test_trad, knn_pred),\n",
        "    'f1': f1_score(y_test_trad, knn_pred)\n",
        "}\n",
        "print(\" KNN trained\")\n",
        "print(knn_results)\n",
        "\n",
        "# Algorithm 2: Isolation Forest (Unsupervised Anomaly Detection)\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "print(\"\\nTraining Isolation Forest...\")\n",
        "iso_model = IsolationForest(contamination=0.5, random_state=42)\n",
        "iso_model.fit(X_train_trad)\n",
        "iso_pred = iso_model.predict(X_test_trad)\n",
        "# Convert to binary (1=normal, -1=anomaly -> 0=normal, 1=anomaly)\n",
        "iso_pred = np.where(iso_pred == 1, 0, 1)\n",
        "\n",
        "iso_results = {\n",
        "    'accuracy': accuracy_score(y_test_trad, iso_pred),\n",
        "    'precision': precision_score(y_test_trad, iso_pred),\n",
        "    'recall': recall_score(y_test_trad, iso_pred),\n",
        "    'f1': f1_score(y_test_trad, iso_pred)\n",
        "}\n",
        "print(\" Isolation Forest trained\")\n",
        "print(iso_results)\n",
        "\n",
        "\n",
        "# Algorithm 3: Support Vector Machine\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "print(\"\\nTraining SVM...\")\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "svm_model.fit(X_train_trad, y_train_trad)\n",
        "svm_pred = svm_model.predict(X_test_trad)\n",
        "\n",
        "svm_results = {\n",
        "    'accuracy': accuracy_score(y_test_trad, svm_pred),\n",
        "    'precision': precision_score(y_test_trad, svm_pred),\n",
        "    'recall': recall_score(y_test_trad, svm_pred),\n",
        "    'f1': f1_score(y_test_trad, svm_pred)\n",
        "}\n",
        "print(\" SVM trained\")\n",
        "print(svm_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDDcUIE7fphm",
        "outputId": "7d0f24ae-fb99-4b6f-95f6-fe22628465af"
      },
      "outputs": [],
      "source": [
        "# For CNN, 2D structure\n",
        "X_cnn = np.array(X_normal + X_abnormal)\n",
        "y_cnn = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_abnormal))])\n",
        "\n",
        "# Split\n",
        "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
        "    X_cnn, y_cnn, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Reshape for CNN (samples, height, width, channels)\n",
        "X_train_cnn = X_train_cnn[..., np.newaxis]\n",
        "X_test_cnn = X_test_cnn[..., np.newaxis]\n",
        "\n",
        "print(f\"CNN Training set: {X_train_cnn.shape}\")\n",
        "print(f\"CNN Test set: {X_test_cnn.shape}\")\n",
        "\n",
        "# Algorithm 4: CNN (Deep Learning)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"\\nTraining CNN...\")\n",
        "cnn_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(40, 100, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = cnn_model.fit(X_train_cnn, y_train_cnn, epochs=20, validation_split=0.2, verbose=0)\n",
        "\n",
        "cnn_pred_probs = cnn_model.predict(X_test_cnn)\n",
        "cnn_pred = (cnn_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "cnn_results = {\n",
        "    'accuracy': accuracy_score(y_test_cnn, cnn_pred),\n",
        "    'precision': precision_score(y_test_cnn, cnn_pred),\n",
        "    'recall': recall_score(y_test_cnn, cnn_pred),\n",
        "    'f1': f1_score(y_test_cnn, cnn_pred),\n",
        "    'loss': history.history['loss'][-1]\n",
        "}\n",
        "print(\" CNN trained\")\n",
        "print(cnn_results)\n",
        "\n",
        "\n",
        "# Implement and train VGG-like model\n",
        "print(\"\\nTraining VGG-like CNN...\")\n",
        "\n",
        "# VGG-like architecture\n",
        "vgg_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(40, 100, 1)),\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "vgg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_vgg = vgg_model.fit(X_train_cnn, y_train_cnn, epochs=20, validation_split=0.2, verbose=0)\n",
        "\n",
        "vgg_pred_probs = vgg_model.predict(X_test_cnn)\n",
        "vgg_pred = (vgg_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "vgg_results = {\n",
        "    'accuracy': accuracy_score(y_test_cnn, vgg_pred),\n",
        "    'precision': precision_score(y_test_cnn, vgg_pred),\n",
        "    'recall': recall_score(y_test_cnn, vgg_pred),\n",
        "    'f1': f1_score(y_test_cnn, vgg_pred),\n",
        "    'loss': history_vgg.history['loss'][-1]\n",
        "}\n",
        "print(\" VGG-like CNN trained\")\n",
        "print(vgg_results)\n",
        "\n",
        "# Implement and train ResNet-like model\n",
        "print(\"\\nTraining ResNet-like CNN...\")\n",
        "\n",
        "def resnet_block(x, filters, kernel_size=3, stride=1):\n",
        "    \"\"\"A simplified ResNet block.\"\"\"\n",
        "    y = keras.layers.Conv2D(filters, kernel_size, strides=stride, padding='same', activation='relu')(x)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.Conv2D(filters, kernel_size, strides=1, padding='same')(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "\n",
        "    if stride != 1 or x.shape[-1] != filters:\n",
        "        x = keras.layers.Conv2D(filters, 1, strides=stride, padding='same')(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    out = keras.layers.Add()([x, y])\n",
        "    out = keras.layers.ReLU()(out)\n",
        "    return out\n",
        "\n",
        "# ResNet-like architecture\n",
        "input_shape = (40, 100, 1)\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "x = keras.layers.Conv2D(32, 7, strides=2, padding='same', activation='relu')(inputs)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "x = resnet_block(x, 32)\n",
        "x = resnet_block(x, 32)\n",
        "\n",
        "x = resnet_block(x, 64, stride=2)\n",
        "x = resnet_block(x, 64)\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dense(64, activation='relu')(x)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "resnet_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "resnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_resnet = resnet_model.fit(X_train_cnn, y_train_cnn, epochs=20, validation_split=0.2, verbose=0)\n",
        "\n",
        "resnet_pred_probs = resnet_model.predict(X_test_cnn)\n",
        "resnet_pred = (resnet_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "resnet_results = {\n",
        "    'accuracy': accuracy_score(y_test_cnn, resnet_pred),\n",
        "    'precision': precision_score(y_test_cnn, resnet_pred),\n",
        "    'recall': recall_score(y_test_cnn, resnet_pred),\n",
        "    'f1': f1_score(y_test_cnn, resnet_pred),\n",
        "    'loss': history_resnet.history['loss'][-1]\n",
        "}\n",
        "print(\" ResNet-like CNN trained\")\n",
        "print(resnet_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%%comment\n",
        "# **Transfer Learning on Tractor Data**\n",
        "Use the pre-trained ResNet-like CNN from MIMII pump data. Freeze early layers and fine-tune on tractor sounds. \n",
        "Assume tractor data is in ../data/tractor/ with similar structure (normal/abnormal WAV files).\n",
        "Save the best fine-tuned model with ModelCheckpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Import necessary for saving\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# After training the ResNet-like model, save it\n",
        "checkpoint = ModelCheckpoint('best_resnet_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "history_resnet = resnet_model.fit(X_train_cnn, y_train_cnn, epochs=20, validation_split=0.2, callbacks=[checkpoint, EarlyStopping(monitor='val_loss', patience=5)], verbose=1)\n",
        "\n",
        "# Load the best saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "resnet_model = load_model('best_resnet_model.h5')\n",
        "print(\"Best pre-trained ResNet-like model saved and loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Load Tractor Dataset (assume similar structure)\n",
        "TRACTOR_BASE_PATH = \"../data/tractor/\"\n",
        "\n",
        "tractor_machine_ids = [\"id_01\", \"id_02\"]  # Replace with your tractor IDs\n",
        "\n",
        "tractor_normal_files = []\n",
        "tractor_abnormal_files = []\n",
        "\n",
        "for mid in tractor_machine_ids:\n",
        "    normal_dir = os.path.join(TRACTOR_BASE_PATH, mid, \"normal\")\n",
        "    abnormal_dir = os.path.join(TRACTOR_BASE_PATH, mid, \"abnormal\")\n",
        "\n",
        "    if os.path.exists(normal_dir):\n",
        "        tractor_normal_files.extend([os.path.join(normal_dir, f) for f in os.listdir(normal_dir) if f.endswith('.wav')])\n",
        "    if os.path.exists(abnormal_dir):\n",
        "        tractor_abnormal_files.extend([os.path.join(abnormal_dir, f) for f in os.listdir(abnormal_dir) if f.endswith('.wav')])\n",
        "\n",
        "print(f\"Tractor normal samples found: {len(tractor_normal_files)}\")\n",
        "print(f\"Tractor abnormal samples found: {len(tractor_abnormal_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Reuse existing extract_mfcc_features function for consistency\n",
        "tractor_mfccs = []\n",
        "tractor_labels = []\n",
        "\n",
        "# Process normal tractor files\n",
        "for file in tractor_normal_files:\n",
        "    mfcc = extract_mfcc_features(file)\n",
        "    tractor_mfccs.append(mfcc)\n",
        "    tractor_labels.append(0)  # 0 for normal\n",
        "\n",
        "# Process abnormal tractor files\n",
        "for file in tractor_abnormal_files:\n",
        "    mfcc = extract_mfcc_features(file)\n",
        "    tractor_mfccs.append(mfcc)\n",
        "    tractor_labels.append(1)  # 1 for abnormal\n",
        "\n",
        "X_tractor = np.array(tractor_mfccs)\n",
        "y_tractor = np.array(tractor_labels)\n",
        "\n",
        "# Split tractor data\n",
        "X_tractor_train, X_tractor_test, y_tractor_train, y_tractor_test = train_test_split(X_tractor, y_tractor, test_size=0.2, random_state=42)\n",
        "print(\"Tractor data split complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Fine-Tuning: Freeze early layers (e.g., first conv blocks)\n",
        "for layer in resnet_model.layers[:5]:  # Freeze first 5 layers\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile for fine-tuning (lower learning rate)\n",
        "resnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for saving best model and early stopping\n",
        "checkpoint = ModelCheckpoint('fine_tuned_resnet_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Fine-tune on tractor data\n",
        "history_fine_tune = resnet_model.fit(\n",
        "    X_tractor_train, y_tractor_train,\n",
        "    epochs=20,\n",
        "    validation_data=(X_tractor_test, y_tractor_test),\n",
        "    callbacks=[checkpoint, early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Load the best fine-tuned model\n",
        "fine_tuned_model = load_model('fine_tuned_resnet_model.h5')\n",
        "print(\"Fine-tuning complete. Best model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Evaluate fine-tuned model\n",
        "fine_pred_probs = fine_tuned_model.predict(X_tractor_test)\n",
        "fine_pred = (fine_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "fine_results = {\n",
        "    'accuracy': accuracy_score(y_tractor_test, fine_pred),\n",
        "    'precision': precision_score(y_tractor_test, fine_pred),\n",
        "    'recall': recall_score(y_tractor_test, fine_pred),\n",
        "    'f1': f1_score(y_tractor_test, fine_pred)\n",
        "}\n",
        "print(\"Fine-Tuned ResNet Results on Tractor Data:\")\n",
        "print(fine_results)\n",
        "\n",
        "# Update comparison table (add to existing results_df)\n",
        "results_df.loc[len(results_df)] = ['Fine-Tuned ResNet', fine_results['accuracy'], fine_results['precision'], fine_results['recall'], fine_results['f1']]\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%%comment\n",
        "# **Prediction on New Tractor Audio Data**\n",
        "Use the fine-tuned ResNet-like CNN to predict the condition of new tractor audio samples. \n",
        "Process the audio with the same MFCC extraction pipeline and output the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Prediction function using the fine-tuned model\n",
        "def predict_tractor_condition(audio_file, model):\n",
        "    # Extract MFCC features (reuse existing function)\n",
        "    mfcc = extract_mfcc_features(audio_file)\n",
        "    mfcc = np.expand_dims(mfcc, axis=0)  # Add batch dimension\n",
        "    mfcc = np.expand_dims(mfcc, axis=-1)  # Add channel dimension (for CNN)\n",
        "\n",
        "    # Predict\n",
        "    pred_prob = model.predict(mfcc, verbose=0)\n",
        "    pred = (pred_prob > 0.5).astype(int)[0][0]  # Threshold at 0.5\n",
        "    confidence = pred_prob[0][0] if pred == 1 else 1 - pred_prob[0][0]  # Confidence score\n",
        "\n",
        "    return pred, confidence\n",
        "\n",
        "# Load the fine-tuned model\n",
        "from tensorflow.keras.models import load_model\n",
        "fine_tuned_model = load_model('fine_tuned_resnet_model.h5')\n",
        "\n",
        "print(\"Prediction function and model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%comment\n",
        "# Example: Predict on new tractor audio files\n",
        "# Replace with paths to new audio files\n",
        "new_audio_files = [\n",
        "    \"../data/tractor/id_01/normal/sample_normal.wav\",\n",
        "    \"../data/tractor/id_02/abnormal/sample_abnormal.wav\"\n",
        "]\n",
        "\n",
        "# Predict for each file\n",
        "for file in new_audio_files:\n",
        "    prediction, confidence = predict_tractor_condition(file, fine_tuned_model)\n",
        "    condition = \"Abnormal\" if prediction == 1 else \"Normal\"\n",
        "    print(f\"File: {file}\")\n",
        "    print(f\"Predicted Condition: {condition}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S3utgRwhsXK"
      },
      "source": [
        "# **Results Comparison Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ngUCsNgK6U",
        "outputId": "a2e73e45-088a-4b81-b2f4-531ffcdfe8c4"
      },
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "results_df = pd.DataFrame({\n",
        "    'Algorithm': ['KNN', 'Isolation Forest', 'SVM', 'CNN', 'VGG-like CNN', 'ResNet-like CNN'],\n",
        "    'Accuracy': [knn_results['accuracy'],\n",
        "                 iso_results['accuracy'], svm_results['accuracy'],\n",
        "                 cnn_results['accuracy'], vgg_results['accuracy'], resnet_results['accuracy']],\n",
        "    'Precision': [knn_results['precision'],\n",
        "                  iso_results['precision'], svm_results['precision'],\n",
        "                  cnn_results['precision'], vgg_results['precision'], resnet_results['precision']],\n",
        "    'Recall': [knn_results['recall'],\n",
        "               iso_results['recall'], svm_results['recall'],\n",
        "               cnn_results['recall'], vgg_results['recall'], resnet_results['recall']],\n",
        "    'F1-Score': [knn_results['f1'],\n",
        "                 iso_results['f1'], svm_results['f1'],\n",
        "                 cnn_results['f1'], vgg_results['f1'], resnet_results['f1']]\n",
        "})\n",
        "\n",
        "results_df = results_df.round(4)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PERFORMANCE COMPARISON ACROSS ALL ALGORITHMS\")\n",
        "print(\"=\"*70)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save table\n",
        "results_df.to_csv('model_comparison_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty76XjV9hjMa"
      },
      "source": [
        "# **Visualize Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "Xp1dgFp5hLRx",
        "outputId": "27171b13-acb3-4540-a63e-59a3a2e9ccc0"
      },
      "outputs": [],
      "source": [
        "# Bar chart comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    ax.bar(results_df['Algorithm'], results_df[metric])\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.set_ylim([0, 1.05]) # Extend y-limit slightly for labels\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(results_df[metric]):\n",
        "        ax.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('algorithm_comparison_updated.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqiXtFOEhbuW"
      },
      "source": [
        "# **Confusion Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "VtrCcR-ShZN9",
        "outputId": "d4198c25-ec02-4262-8d7c-223b80a9d9bf"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "predictions = [knn_pred, iso_pred, svm_pred, cnn_pred, vgg_pred, resnet_pred]\n",
        "names = [ 'KNN', 'Isolation Forest', 'SVM', 'CNN', 'VGG-like CNN', 'ResNet-like CNN']\n",
        "\n",
        "for idx, (pred, name) in enumerate(zip(predictions, names)):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    # Use y_test_trad for the first 3 models, y_test_cnn for the last 3\n",
        "    y_true = y_test_trad if idx < 3 else y_test_cnn\n",
        "    cm = confusion_matrix(y_true, pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "    ax.set_title(f'{name} Confusion Matrix')\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f429cd66"
      },
      "source": [
        "# **Interpretation of Results**\n",
        "\n",
        "Based on the performance metrics and confusion matrices, the supervised learning approaches (KNN, SVM, and the CNN variants) demonstrated superior performance in detecting abnormal pump sounds compared to the unsupervised Isolation Forest.\n",
        "\n",
        "The traditional models (KNN, SVM) using statistical features and the deep learning models (VGG-like CNN, ResNet-like CNN) processing 2D MFCCs both achieved high performance. KNN had perfect precision, while VGG-like and ResNet-like CNNs showed strong recall.\n",
        "\n",
        "Overall, KNN, SVM, VGG-like CNN, and ResNet-like CNN appear to be the most promising models for this task based on these initial results. Further analysis of the confusion matrices can reveal specific error types to inform model selection for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update comparison table (add to existing results_df)\n",
        "results_df.loc[len(results_df)] = ['Fine-Tuned ResNet', fine_results['accuracy'], fine_results['precision'], results_df['recall'], results_df['f1']]\n",
        "print(results_df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tractorcarenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
